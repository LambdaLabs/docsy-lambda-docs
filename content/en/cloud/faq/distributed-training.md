---
title: "How can I perform distributed training on Lambda GPU Cloud?"
type: docs
---

Distributed training can be performed across multiple instances using a
framework like Horovod.

We recommend only using the same instance type for each node when clustering
instances.

The maximum inter-node connectivity is 10 Gbps.
